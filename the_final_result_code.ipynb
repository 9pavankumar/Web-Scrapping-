{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7107249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1pava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1pava\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install syllables\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import syllables  # Add this line for syllable counting\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96f918",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "beautifulsoup4: For parsing HTML content from websites.\n",
    "\n",
    "requests: To make HTTP requests and download webpage content.\n",
    "\n",
    "openpyxl: To read and write data to Excel files.\n",
    "\n",
    "nltk: For natural language processing tasks like sentiment analysis and text preprocessing.\n",
    "\n",
    "textstat: For calculating readability statistics like FOG Index and average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dbf66f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Replace multiple spaces with a single space\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Function to calculate derived variables\n",
    "def calculate_derived_variables(text):\n",
    "    # Perform text analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    positive_score = sum(1 for word in blob.words if word.lower() in positive_words)\n",
    "    negative_score = sum(1 for word in blob.words if word.lower() in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(blob.words) + 0.000001)\n",
    "\n",
    "    # Readability Analysis\n",
    "    avg_sentence_length = len(blob.words) / len(blob.sentences)\n",
    "    percentage_complex_words = sum(1 for word in blob.words if syllables.estimate(word) > 2) / len(blob.words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(blob.words) / len(blob.sentences)\n",
    "\n",
    "    # Additional Variables\n",
    "    complex_word_count = sum(1 for word in blob.words if syllables.estimate(word) > 2)\n",
    "    word_count = len(blob.words)\n",
    "    syllable_per_word = sum(syllables.estimate(word) for word in blob.words) / len(blob.words)\n",
    "    personal_pronouns = sum(1 for word in blob.words if word.lower() in personal_pronouns_list)\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / len(blob.words)\n",
    "\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score, \\\n",
    "           avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "           complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e485c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from the input file\n",
    "input_file_path = r\"C:\\Users\\1pava\\Documents\\ALL PROJECTS\\Web Scrapping\\Output Data Structure.xlsx\"\n",
    "df = pd.read_excel(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ecc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Positive and Negative words\n",
    "with open(r\"C:\\Users\\1pava\\Documents\\ALL PROJECTS\\Web Scrapping\\negative-words.txt\", 'r') as file:\n",
    "    negative_words = set(file.read().splitlines())\n",
    "\n",
    "with open(r\"C:\\Users\\1pava\\Documents\\ALL PROJECTS\\Web Scrapping\\positive-words.txt\", 'r') as file:\n",
    "    positive_words = set(file.read().splitlines())\n",
    "\n",
    "# Load Personal Pronouns list\n",
    "personal_pronouns_list = ['i', 'we', 'my', 'ours', 'us']\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "output_df = pd.DataFrame(columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a558be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the dictionaries\n",
    "output_data = []\n",
    "\n",
    "# Iterate through each row in the input DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "         # Extract the title\n",
    "        title_element = soup.find('title')\n",
    "        article_title = title_element.text if title_element else \"No title found\"\n",
    "\n",
    "        # Extract the main content of the article (adjust the class or tag accordingly)\n",
    "        main_content_element = soup.find('div', class_='td-post-content')  # Adjust the class accordingly\n",
    "        if main_content_element:\n",
    "            # Remove unwanted elements (e.g., headers, footers)\n",
    "            unwanted_elements = main_content_element.find_all(['header', 'footer'])  # Add more if needed\n",
    "            for unwanted_element in unwanted_elements:\n",
    "                unwanted_element.decompose()\n",
    "\n",
    "            # Find all <p> tags within the specified div\n",
    "            paragraphs = main_content_element.find_all('p')\n",
    "\n",
    "            # Concatenate the text from all <p> tags to form the complete content\n",
    "            article_text = ' '.join([paragraph.get_text() for paragraph in paragraphs])\n",
    "\n",
    "            # Clean and preprocess the text\n",
    "            cleaned_text = clean_text(article_text)\n",
    "\n",
    "            # Calculate derived variables\n",
    "            positive_score, negative_score, polarity_score, subjectivity_score, \\\n",
    "            avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "            complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length = calculate_derived_variables(cleaned_text)\n",
    "\n",
    "            # Append the results to the list\n",
    "            output_data.append({\n",
    "                'URL_ID': row['URL_ID'],\n",
    "                'URL': url,\n",
    "                'POSITIVE SCORE': positive_score,\n",
    "                'NEGATIVE SCORE': negative_score,\n",
    "                'POLARITY SCORE': polarity_score,\n",
    "                'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "                'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "                'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "                'FOG INDEX': fog_index,\n",
    "                'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "                'COMPLEX WORD COUNT': complex_word_count,\n",
    "                'WORD COUNT': word_count,\n",
    "                'SYLLABLE PER WORD': syllable_per_word,\n",
    "                'PERSONAL PRONOUNS': personal_pronouns,\n",
    "                'AVG WORD LENGTH': avg_word_length\n",
    "            })\n",
    "\n",
    "# Create the output DataFrame from the list of dictionaries\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the output DataFrame to a new Excel file\n",
    "output_file_path = r\"C:\\Users\\1pava\\Documents\\ALL PROJECTS\\Web Scrapping\\Output_Result.xlsx\"\n",
    "output_df.to_excel(output_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
